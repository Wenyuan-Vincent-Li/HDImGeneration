import torch.nn as nn
import torch.optim as optim
import torch.utils.data
from Training import functions
from Training.imresize import imresize
import matplotlib.pyplot as plt


def train_single_scale(dataloader, netD, netG, reals, Gs, Zs, in_s, NoiseAmp, opt):
    '''
    :param netD: currD
    :param netG: currG
    :param reals: a list of image pyramid ## TODO: you can just pass image shape here
    :param Gs: list of prev netG
    :param Zs: [[Gaussian noise [1, 3, 36, 36]], ]
    :param in_s: 0-> all zero [1, 3, 26, 26]
    :param NoiseAmp: [] -> [1]
    :param opt: config
    :return:
    '''

    real = reals[opt.scale_num]  # find the current level image xn
    opt.nzx = real[0]  # +(opt.ker_size-1)*(opt.num_layer)
    opt.nzy = real[1]  # +(opt.ker_size-1)*(opt.num_layer)
    opt.receptive_field = opt.ker_size + ((opt.ker_size - 1) * (opt.num_layer - 1)) * opt.stride
    pad_noise = int(((opt.ker_size - 1) * opt.num_layer) / 2)  # 5
    pad_image = int(((opt.ker_size - 1) * opt.num_layer) / 2)  # 5

    m_noise = nn.ZeroPad2d(int(pad_noise))
    m_image = nn.ZeroPad2d(int(pad_image))

    alpha = opt.alpha  # 10

    # setup optimizer
    optimizerD = optim.Adam(netD.parameters(), lr=opt.lr_d, betas=(opt.beta1, 0.999))
    optimizerG = optim.Adam(netG.parameters(), lr=opt.lr_g, betas=(opt.beta1, 0.999))
    schedulerD = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerD, milestones=[160], gamma=opt.gamma)
    schedulerG = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerG, milestones=[160], gamma=opt.gamma)

    errD2plot = []
    errG2plot = []
    D_real2plot = []
    D_fake2plot = []
    z_opt2plot = []

    for epoch in range(opt.niter):  # niter = 2000
        if Gs == []:
            noise_ = functions.generate_noise([1, opt.nzx, opt.nzy], opt.batchSize)  # [None, 1, 32, 32]
            noise_ = m_noise(noise_.expand(opt.batchSize, opt.label_nc, opt.nzx, opt.nzy))  # [None, 3, 42, 42]
            ## Noise_: for generated false samples and input them to discriminator
        else:
            noise_ = functions.generate_noise([1, opt.nzx, opt.nzy], opt.batchSize)
            noise_ = m_noise(noise_.expand(opt.batchSize, opt.label_nc, opt.nzx, opt.nzy))  # scale = 1, noise_ [None, 3, 74, 74]

        for j, data in enumerate(dataloader):
            data['label'] = data['label'].to(opt.device)
            data['label'] = functions.mask2onehot(data['label'], opt.label_nc)
            ############################
            # (1) Update D network: maximize D(x) + D(G(z))
            ###########################

            # train with real
            netD.zero_grad()
            output = netD(data['label']).to(
                opt.device)  # real [None, 3, 32, 32] -> output [None, 1, 22, 22]
            errD_real = -output.mean()  # -a W-GAN loss
            errD_real.backward(retain_graph=True)
            D_x = -errD_real.item()

            # train with fake
            if (j == 0) & (epoch == 0):  # first iteration training in this level
                if Gs == []:
                    prev = torch.full([opt.batchSize, opt.label_nc, opt.nzx, opt.nzy], 0, device=opt.device)
                    in_s = prev  # full of 0 [None, 3, 32, 32]
                    prev = m_image(prev)  # [None, 3, 42, 42]
                    opt.noise_amp = 1
                else:
                    prev = draw_concat(Gs, reals, NoiseAmp, in_s, 'rand', m_noise, m_image,
                                       opt)
                    ## given a new noise, prev is a image generated by previous Generator with bilinear upsampling [1, 3, 33, 33]

                    # z_prev = draw_concat(Gs, Zs_prime, fixed_data_label, reals, NoiseAmp, in_s, 'rec', m_noise, m_image, opt) ## [1, 3, 33, 33]
                    ## z_prev is a image generated using a specific set of z group, for the purpose determine the intensity of noise
                    criterion = nn.MSELoss()
                    RMSE = torch.sqrt(criterion(data['label'], prev))

                    opt.noise_amp = opt.noise_amp_init * RMSE
                    prev = m_image(prev)  ## [1, 3, 43, 43]
            else:
                prev = draw_concat(Gs, reals, NoiseAmp, in_s, 'rand', m_noise, m_image, opt)
                ## Sample another image generated by the previous generator
                prev = m_image(prev)

            if Gs == []:
                noise = noise_  ## Gausiaan noise for generating image [None, 3, 42, 42]
            else:
                noise = opt.noise_amp * noise_ + prev  ## [None, 3, 43, 43] new noise is equal to the prev generated image plus the gaussian noise.

            fake = netG(noise.detach(), prev)  # [None, 3, 32, 32] the same size with the input image
            # detach() make sure that the gradients don't go to the noise.
            # prev:[None, 3, 42, 42] -> [None, 3, 43, 43] first step prev = 0, second step prev = a image generated by previous Generator with bilinaer upsampling
            output = netD(fake.detach())  # output shape [1, 1, 16, 16] -> [1, 1, 23, 23]
            errD_fake = output.mean()
            errD_fake.backward(retain_graph=True)
            D_G_z = output.mean().item()

            gradient_penalty = functions.calc_gradient_penalty_mask(netD, data['label'], fake,
                                                               opt.lambda_grad)
            gradient_penalty.backward()
            errD = errD_real + errD_fake + gradient_penalty
            optimizerD.step()

            errD2plot.append(errD.detach())  ## errD for each iteration

            ############################
            # (2) Update G network: maximize D(G(z))
            ###########################
            netG.zero_grad()
            output = netD(fake)
            # D_fake_map = output.detach()
            errG = -output.mean()  # Generator want to make output as large as possible.
            errG.backward(retain_graph=True)

            if alpha != 0:  ## alpha = 10 calculate the reconstruction loss
                loss = nn.MSELoss()
                rec_loss = alpha * loss(fake, data['label'])
                rec_loss.backward()
                rec_loss = rec_loss.detach()
            else:
                rec_loss = 0
            optimizerG.step()

        ## for every epoch, do the following:
        errG2plot.append(errG.detach() + rec_loss)  ## ErrG for each iteration
        D_real2plot.append(D_x)  ##  discriminator loss on real
        D_fake2plot.append(D_G_z)  ## discriminator loss on fake
        z_opt2plot.append(rec_loss)  ## reconstruction loss
        if epoch % 25 == 0 or epoch == (opt.niter - 1):
            print('scale %d:[%d/%d]' % (opt.scale_num, epoch, opt.niter))

        if epoch % 25 == 0 or epoch == (opt.niter - 1):
            plt.imsave('%s/fake_mask_%d.png' % (opt.outf, epoch),
                       functions.convert_mask_np(torch.argmax(input = fake.detach(), dim = 1, keepdim = True)))
            plt.imsave('%s/fake_mask_real_%d.png' % (opt.outf, epoch),
                       functions.convert_mask_np(torch.argmax(input = data['label'], dim = 1, keepdim = True)))

        schedulerD.step()
        schedulerG.step()
    z_opt = 1
    functions.save_networks(netG, netD, z_opt, opt)  ## save netG, netD, z_opt, opt is used to parser output path
    return z_opt, in_s, netG
    # first scale z_opt: generated Gaussion noise [1, 3, 36, 36], in_s = all 0 [1, 3, 26, 26] netG: generator
    # second scale z_opt: all zeros [1, 3, 43, 43], in_s = [1, 3, 26, 26], all zeros


def draw_concat(Gs, reals, NoiseAmp, in_s, mode, m_noise, m_image, opt):
    '''
    :param Gs: [G0]
    # :param Zs: [[Gaussion Noise (1, 3, 36, 36)]]
    :param reals: [image pyramid] only used to represent the image shape
    :param NoiseAmp: [1]
    :param in_s: all zeros [1, 3, 26, 26]
    :param mode: 'rand'
    :param m_noise:
    :param m_image:
    :param opt:
    :return:
    '''
    G_z = in_s[:opt.batchSize, :, :, :]  # [None, 3, 26, 26] all zeros, image input for the corest level
    if len(Gs) > 0:
        if mode == 'rand' or mode == 'rec':
            count = 0
            # pad_noise = int(((opt.ker_size-1)*opt.num_layer)/2)
            for G, real_curr, real_next, noise_amp in zip(Gs, reals, reals[1:], NoiseAmp):
                if count == 0:
                    z = functions.generate_noise([1, real_curr[0], real_curr[1]],
                                                 opt.batchSize)
                    z = z.expand(opt.batchSize, opt.label_nc, z.shape[2], z.shape[3])
                else:
                    z = functions.generate_noise([opt.label_nc, real_curr[0], real_curr[1]], opt.batchSize)
                ## z [None, 3, 32, 32]
                z = m_noise(z)  ## z [1, 3, 42, 42]
                G_z = G_z[:, :, 0:real_curr[0], 0:real_curr[1]]  ## G_z [None, 3, 32, 32]
                G_z = m_image(G_z)  ## G_z [None, 3, 42, 42] all zeros
                z_in = noise_amp * z + G_z  ## [None, 3, 42, 42] Gaussian noise

                ## TODO: check the size of noise and mask
                ## resize mask to current level
                G_z = G(z_in.detach(), G_z)  ## [1, 3, 26, 26] output of previous generator
                G_z = imresize(G_z, real_next[1] / real_curr[1], opt)  ## output upsampling (bilinear) [1, 3, 34, 34]
                G_z = G_z[:, :, 0:real_next[0],
                      0:real_next[1]]  ## resize the image to be compatible with current G [1, 3, 33, 33]
                count += 1
    return G_z
